{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo code for the email server honeypot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 16:51:51 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import dpp\n",
    "import utils\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "\n",
    "# Set up logger/arg parser\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "MODEL_CLASSES = {\"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer)}\n",
    "\n",
    "model_name_or_path = './models/gpt2-email-body'\n",
    "args = utils.get_parser(model_name_or_path = model_name_or_path)\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "\n",
    "logger.warning(\n",
    "    \"device: %s, n_gpu: %s, 16-bits training: %s\",\n",
    "    args.device,\n",
    "    args.n_gpu,\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    "utils.set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LogNormMix-Net TPP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "dataset_name = 'enron_email_dataset'  # run dpp.data.list_datasets() to see the list of available datasets\n",
    "\n",
    "# Model config\n",
    "## Marks\n",
    "use_src_marks = True              # Use source marks\n",
    "src_mark_embedding_size = 24          # Size of the src mark embedding (used as RNN input)\n",
    "use_dst_marks = True                  # Use destination marks\n",
    "dst_mark_embedding_size = 24          # Size of the dst mark embedding (used as RNN input)\n",
    "shared_mark_embedding = False          # Should the source and destination marks share an embedding layer (note, embedding sizes must be the same, and have the same range)\n",
    "\n",
    "context_size = 64                # Size of the RNN hidden vector\n",
    "num_mix_components = 30           # Number of components for a mixture model\n",
    "rnn_type = \"LSTM\"                  # What RNN to use as an encoder {\"RNN\", \"GRU\", \"LSTM\"}\n",
    "meta_embedding_size = 16\n",
    "num_meta_classes = 3\n",
    "meta_type = 'basic'\n",
    "\n",
    "# Training config\n",
    "batch_size = 50       # Number of sequences in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_end: 86\n",
      "val_end: 115\n"
     ]
    }
   ],
   "source": [
    "dataset = dpp.data.load_dataset(dataset_name)\n",
    "d_train, d_val, d_test = dataset.train_val_test_split(seed=seed)\n",
    "\n",
    "dl_train = d_train.get_dataloader(batch_size=batch_size, shuffle=True)\n",
    "dl_val = d_val.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "dl_test = d_test.get_dataloader(batch_size=batch_size, shuffle=False)\n",
    "\n",
    "mean_log_inter_time, std_log_inter_time = d_train.get_inter_time_statistics()\n",
    "\n",
    "tpp_model = dpp.models.LogNormMixNet(\n",
    "    use_src_marks=use_src_marks,\n",
    "    use_dst_marks=use_dst_marks,\n",
    "    num_src_marks=d_train.num_src_marks,\n",
    "    num_dst_marks=d_train.num_dst_marks,\n",
    "    num_meta_classes=num_meta_classes,\n",
    "    meta_type=meta_type,\n",
    "    mean_log_inter_time=mean_log_inter_time,\n",
    "    std_log_inter_time=std_log_inter_time,\n",
    "    context_size=context_size,\n",
    "    src_mark_embedding_size=src_mark_embedding_size,\n",
    "    dst_mark_embedding_size=dst_mark_embedding_size,\n",
    "    shared_mark_embedding = shared_mark_embedding,\n",
    "    rnn_type=rnn_type,\n",
    "    num_mix_components=num_mix_components,\n",
    "    meta_embedding_size=meta_embedding_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOAD MODEL PARAMS\n",
    "tpp_model.load_state_dict(torch.load('./models/enron-event-predict-model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email generation\n",
    "#### **1. Data Preprocessing:**\n",
    "   a) create training dataset for the intensity-free TPP model  \n",
    "   b) create training dataset for finetuning the huggingface GPT2 model  \n",
    "#### **2. Train the intensity-free TPP model:** LogNormMix-Net model.  \n",
    "#### **3. Fine tune huggingface/transformers GPT2 model** on the Enron email text.  \n",
    "#### **4. Generate email traffic:**   \n",
    "   For each event:  \n",
    "   i)  generate the timestamp, sender & recip set using the TPP model  \n",
    "   ii) sample email thread type (new-thread, reply, fwd) based on sent email counts from training data.  \n",
    "   iii) if reply/fwd: choose the most recent email tread appropriate for the recipients and email type.  \n",
    "   iv) generate email text by passing the existing email thread to the GPT2 model and asking it to generate 2 sentences of text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   Model name './models/gpt2-email-body' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming './models/gpt2-email-body' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   Didn't find file ./models/gpt2-email-body/tokenizer.json. We won't load it.\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   loading file ./models/gpt2-email-body/vocab.json\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   loading file ./models/gpt2-email-body/merges.txt\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   loading file ./models/gpt2-email-body/added_tokens.json\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   loading file ./models/gpt2-email-body/special_tokens_map.json\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   loading file ./models/gpt2-email-body/tokenizer_config.json\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "03/29/2022 17:09:04 - INFO - transformers.tokenization_utils -   Adding Enron to the vocabulary\n",
      "03/29/2022 17:09:04 - INFO - transformers.configuration_utils -   loading configuration file ./models/gpt2-email-body/config.json\n",
      "03/29/2022 17:09:04 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50258\n",
      "}\n",
      "\n",
      "03/29/2022 17:09:04 - INFO - transformers.modeling_utils -   loading weights file ./models/gpt2-email-body/pytorch_model.bin\n",
      "03/29/2022 17:09:09 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "03/29/2022 17:09:09 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models/gpt2-email-body.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and tokenizer\n",
    "args.model_type = args.model_type.lower()\n",
    "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "model = model_class.from_pretrained(args.model_name_or_path)\n",
    "model.to(args.device)\n",
    "\n",
    "if args.fp16:\n",
    "    model.half()\n",
    "\n",
    "args.length = utils.adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use per-person proportions of new-thread, reply and forward emails from the training dataset\n",
    "message_type_props = pd.read_csv('../data/data_for_simulation/message_count_types.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User faker generated employee identities\n",
    "Using fake names is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faker_db = pd.read_csv('../data/data_for_simulation/faker_employee_names.csv')\n",
    "\n",
    "# Create id to name map\n",
    "employee_name_map = dict(zip(faker_db.id, faker_db.first_name)) \n",
    "employee_surname_map = dict(zip(faker_db.id, faker_db.last_name))\n",
    "employee_email_map = dict(zip(faker_db.id, faker_db.email)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Elizabeth',\n",
       " 1: 'Joshua',\n",
       " 2: 'Haley',\n",
       " 3: 'Allison',\n",
       " 4: 'Nathan',\n",
       " 5: 'Faith',\n",
       " 6: 'Sandra',\n",
       " 7: 'John',\n",
       " 8: 'Albert',\n",
       " 9: 'Michael',\n",
       " 10: 'Michelle',\n",
       " 11: 'Rebekah',\n",
       " 12: 'Steve',\n",
       " 13: 'Linda',\n",
       " 14: 'Andrew',\n",
       " 15: 'Brian',\n",
       " 16: 'William',\n",
       " 17: 'Jill',\n",
       " 18: 'John',\n",
       " 19: 'Katelyn',\n",
       " 20: 'Tiffany',\n",
       " 21: 'Omar',\n",
       " 22: 'Vincent',\n",
       " 23: 'Kayla',\n",
       " 24: 'Glenn',\n",
       " 25: 'Lisa',\n",
       " 26: 'Debbie',\n",
       " 27: 'Jill',\n",
       " 28: 'Dylan',\n",
       " 29: 'Wendy',\n",
       " 30: 'Kenneth',\n",
       " 31: 'Tanya',\n",
       " 32: 'Nicholas',\n",
       " 33: 'Julie',\n",
       " 34: 'Aaron',\n",
       " 35: 'Lauren',\n",
       " 36: 'Tonya',\n",
       " 37: 'Travis',\n",
       " 38: 'Christina',\n",
       " 39: 'William',\n",
       " 40: 'Samuel',\n",
       " 41: 'Toni',\n",
       " 42: 'Erik',\n",
       " 43: 'Paul',\n",
       " 44: 'Ronald',\n",
       " 45: 'Angela',\n",
       " 46: 'Joe',\n",
       " 47: 'Benjamin',\n",
       " 48: 'Michelle',\n",
       " 49: 'Laura',\n",
       " 50: 'Adam',\n",
       " 51: 'Adrienne',\n",
       " 52: 'Laura',\n",
       " 53: 'William'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_name_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run generation: sample recipients and email thread, and generate email content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/work/d61-decaas/work/moo331/miniconda3/envs/IFLeurosp/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "03/29/2022 17:09:53 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:09:56 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:09:59 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:02 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:06 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:09 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:12 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:15 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:18 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:21 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:24 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:27 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:30 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:33 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:37 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:40 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:10:49 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:52 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:55 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:10:58 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:01 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:04 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:07 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:10 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:13 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:17 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:20 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:23 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:26 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:29 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:32 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:35 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:38 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:41 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:11:45 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:11:48 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:11:51 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:11:54 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:11:57 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:00 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:03 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:06 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:09 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:12 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:15 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:18 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:21 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:25 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:28 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:12:31 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:34 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:37 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:40 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:49 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:12:52 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:56 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:12:59 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:02 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:05 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:08 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:11 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:14 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:17 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:21 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:13:24 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:27 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:30 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:32 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:35 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:38 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:41 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:44 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:48 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:13:51 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:54 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:13:57 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:00 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:03 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:06 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:09 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:12 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:15 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:14:18 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:21 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:24 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:28 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:31 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:34 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:37 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:40 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:14:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:49 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:51 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:54 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:14:57 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:00 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:03 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:06 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:09 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:12 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:15 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:18 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:21 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:24 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:28 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:31 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:34 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:37 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:40 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n",
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:15:49 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:51 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:54 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:15:57 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:00 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:03 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:06 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:09 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:12 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:15 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:18 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:21 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:25 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:28 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n",
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:31 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:34 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:37 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:40 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:50 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:53 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:16:56 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:16:59 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:02 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:05 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:08 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:11 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:14 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:17 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:20 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:23 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:26 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:30 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:33 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:17:36 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:39 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:42 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:45 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:48 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:51 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:17:54 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:17:57 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:00 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:04 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:07 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:10 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:13 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:16 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:19 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:18:22 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:25 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:28 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:31 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:34 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:37 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:40 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:49 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:53 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:56 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:18:59 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:02 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:05 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:08 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:11 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:14 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:17 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:20 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:24 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:27 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:30 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:33 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:36 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:39 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:43 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:46 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:49 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:52 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:55 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:19:58 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:01 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:04 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:08 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:20:11 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:14 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:17 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:20:20 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:23 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:20:26 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:29 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:32 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:35 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:38 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:41 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:44 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:20:47 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:20:51 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/29/2022 17:20:54 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "03/29/2022 17:20:57 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "# Generate emails\n",
    "num_hours = 24*7*2\n",
    "## put these in the config yaml?\n",
    "subjects = \"../data/data_for_simulation/subjects_by_ID.csv\"\n",
    "generated_emails_db, generated_emails_list, recipients_db = \\\n",
    "    utils.run_generation(num_hours, model, tpp_model, args, tokenizer, employee_name_map, employee_email_map, message_type_props, subjects)\n",
    "## TODO: re-incorporate the times, and convert to DTs?\n",
    "#emails['dt'] = datetime(2013, 3, 4, 0, 59).astimezone() + pd.to_timedelta(emails['scaled_ts'],'s')\n",
    "#emails['datetime'] = emails['dt'].apply(lambda x: x.strftime('%a')) + \"   \"+ emails['dt'].apply(lambda x: datetime.strftime(x, '%d/%m/%y %I:%M %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(generated_emails_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_emails_db.to_csv(\"../data/generated_output/emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipients_db.to_csv(\"../data/generated_output/generated_recipients_db.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_to_preview = generated_emails_db[generated_emails_db.thread_length==2]['thread_id'].tolist()\n",
    "thread = iter(threads_to_preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = next(thread)\n",
    "print(f\"EMAIL THREAD #: {thread_id}\")\n",
    "print(f\"Sent: {generated_emails_db[(generated_emails_db.thread_id==thread_id)].tail(1)['date-time'].iloc[0]}\")\n",
    "print(f\"Subject: {generated_emails_db[(generated_emails_db.thread_id==thread_id)]['subject'].iloc[0]}\")\n",
    "print(generated_emails_db[(generated_emails_db.thread_id==thread_id)].tail(1)['full_email_thread'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
